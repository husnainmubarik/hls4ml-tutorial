
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import numpy as np


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1
from callbacks import all_callbacks


import plotting
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

model = tf.keras.applications.MobileNet(input_shape=(128,128,3),alpha=0.25,
                                                     include_top=False)

allLayers = model.layers
for layer in allLayers:
    print('layer dir : ', layer.name)

import hls4ml
config = hls4ml.utils.config_from_keras_model(model, granularity='model')
print(config)
hls_model = hls4ml.converters.convert_from_keras_model(model, hls_config=config, output_dir='test_model/hls4ml_prj')




hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)



hls_model.compile()
#y_hls = hls_model.predict(X_test)


# ## Compare
# That was easy! Now let's see how the performance compares to Keras:

# In[ ]:


#print("Keras  Accuracy: {}".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))
#print("hls4ml Accuracy: {}".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))

#fig, ax = plt.subplots(figsize=(9, 9))
#_ = plotting.makeRoc(y_test, y_keras, le.classes_)
#plt.gca().set_prop_cycle(None) # reset the colors
#_ = plotting.makeRoc(y_test, y_hls, le.classes_, linestyle='--')

#from matplotlib.lines import Line2D
#lines = [Line2D([0], [0], ls='-'),
#         Line2D([0], [0], ls='--')]
#from matplotlib.legend import Legend
#leg = Legend(ax, lines, labels=['keras', 'hls4ml'],
#            loc='lower right', frameon=False)
#ax.add_artist(leg)


# ## Synthesize
# Now we'll actually use Vivado HLS to synthesize the model. We can run the build using a method of our `hls_model` object.
# After running this step, we can integrate the generated IP into a workflow to compile for a specific FPGA board.
# In this case, we'll just review the reports that Vivado HLS generates, checking the latency and resource usage.
# 
# **This can take several minutes.**

# In[ ]:


#hls_model.build()


# ## Check the reports
# Print out the reports generated by Vivado HLS. Pay attention to the Latency and the 'Utilization Estimates' sections

# In[ ]:


#hls4ml.report.read_vivado_report('model_1/hls4ml_prj/')


# ## Exercise
# Since `ReuseFactor = 1` we expect each multiplication used in the inference of our neural network to use 1 DSP. Is this what we see? (Note that the Softmax layer should use 5 DSPs, or 1 per class)
# Calculate how many multiplications are performed for the inference of this network...
# (We'll discuss the outcome)

# In[ ]:




